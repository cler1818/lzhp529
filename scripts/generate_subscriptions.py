#!/usr/bin/env python3
"""
è‡ªåŠ¨è®¢é˜…ç”Ÿæˆå™¨ - ç»ˆæç®€åŒ–ç‰ˆ
æ”¯æŒä»å¤‡æ³¨ä¸­æå–åˆ†ç»„ä¿¡æ¯ï¼Œä¸ºæ¯ä¸ªè®¢é˜…é“¾æ¥åˆ›å»ºç‹¬ç«‹ç­–ç•¥ç»„
ç»Ÿä¸€ä½¿ç”¨æ··åˆç«¯å£7890ï¼Œç­–ç•¥ç»„æåº¦ç®€åŒ–
"""

import os
import re
import base64
import json
import requests
import yaml
from datetime import datetime, timezone, timedelta
from urllib.parse import urlparse, parse_qs, unquote
import time
import shutil

def get_beijing_time():
    """è·å–ä¸œå…«åŒºåŒ—äº¬æ—¶é—´"""
    utc_now = datetime.utcnow()
    beijing_tz = timezone(timedelta(hours=8))
    beijing_time = utc_now.replace(tzinfo=timezone.utc).astimezone(beijing_tz)
    return beijing_time.strftime('%Y-%m-%d %H:%M:%S')

def extract_remark_from_comment(comment_line):
    """ä»æ³¨é‡Šè¡Œä¸­æå–å¤‡æ³¨ä¿¡æ¯"""
    if not comment_line or not isinstance(comment_line, str):
        return None
    
    # ç§»é™¤æ³¨é‡Šç¬¦å·å’Œç©ºæ ¼
    comment_line = comment_line.strip()
    if comment_line.startswith('#'):
        comment_line = comment_line[1:].strip()
    
    # å¦‚æœä¸ºç©ºæˆ–åªæœ‰#ï¼Œè¿”å›None
    if not comment_line:
        return None
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·ã€ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦ä½œä¸ºæ–­ç‚¹
    break_pattern = r'[\s,.;:!?ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿã€\u3000]'
    
    match = re.search(break_pattern, comment_line)
    if match:
        # è·å–æ–­ç‚¹å‰çš„æ–‡æœ¬
        remark = comment_line[:match.start()].strip()
    else:
        # å¦‚æœæ²¡æœ‰æ–­ç‚¹å­—ç¬¦ï¼Œä½¿ç”¨æ•´ä¸ªæ³¨é‡Š
        remark = comment_line.strip()
    
    # æ¸…ç†å¤‡æ³¨ï¼šç§»é™¤å¯èƒ½çš„é¢å¤–ç¬¦å·
    remark = remark.strip(' -_')
    
    # å¦‚æœå¤‡æ³¨é•¿åº¦è¶…è¿‡20ä¸ªå­—ç¬¦ï¼Œæˆªæ–­
    if len(remark) > 20:
        remark = remark[:20]
    
    return remark if remark else None

def parse_source_file(filepath):
    """è§£ææºæ–‡ä»¶ï¼Œæå–å¸¦å¤‡æ³¨çš„é“¾æ¥"""
    results = []
    current_remark = None
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            
            if not line:
                current_remark = None
                continue
            
            if line.startswith('#'):
                # æå–å¤‡æ³¨
                remark = extract_remark_from_comment(line)
                if remark:
                    current_remark = remark
                continue
            
            # éæ³¨é‡Šè¡Œï¼Œä¸”æ˜¯URL
            if line and not line.startswith('#') and re.match(r'^https?://', line):
                results.append({
                    'url': line,
                    'remark': current_remark
                })
                current_remark = None
    
    except Exception as e:
        print(f"è§£ææºæ–‡ä»¶å¤±è´¥: {e}")
    
    return results

def safe_decode_base64(data):
    """å®‰å…¨è§£ç Base64æ•°æ®"""
    if not data:
        return None
    
    data = str(data).strip()
    data = data.replace('\n', '').replace('\r', '')
    
    missing_padding = len(data) % 4
    if missing_padding:
        data += '=' * (4 - missing_padding)
    
    try:
        return base64.b64decode(data).decode('utf-8', errors='ignore')
    except:
        try:
            return base64.urlsafe_b64decode(data).decode('utf-8', errors='ignore')
        except:
            return None

def clean_config(config):
    """æ¸…ç†é…ç½®ï¼Œç§»é™¤ç©ºå€¼å’Œæ— æ•ˆå­—æ®µ"""
    if not isinstance(config, dict):
        return config
    
    cleaned = {}
    for key, value in config.items():
        if value is None or value == '':
            continue
        
        if isinstance(value, (list, dict)) and len(value) == 0:
            continue
        
        if isinstance(value, dict):
            cleaned_value = clean_config(value)
            if cleaned_value:
                cleaned[key] = cleaned_value
        elif isinstance(value, list):
            cleaned_list = [clean_config(item) for item in value if clean_config(item) is not None]
            if cleaned_list:
                cleaned[key] = cleaned_list
        else:
            cleaned[key] = value
    
    return cleaned

def parse_hysteria2(url, remark=None):
    """è§£æHysteria2é“¾æ¥"""
    try:
        url = url[11:]  # ç§»é™¤ hysteria2://
        
        name = ""
        if '#' in url:
            url, fragment = url.split('#', 1)
            name = unquote(fragment)
        
        if '@' in url:
            auth_part, server_part = url.split('@', 1)
            password = auth_part
        else:
            return None
        
        server = ""
        port = 443
        query_params = {}
        
        if '?' in server_part:
            server_port_part, query_str = server_part.split('?', 1)
            query_params = parse_qs(query_str)
        else:
            server_port_part = server_part
        
        if ':' in server_port_part:
            server, port_str = server_port_part.split(':', 1)
            port = int(port_str)
        else:
            server = server_port_part
        
        # æ·»åŠ å¤‡æ³¨å‰ç¼€
        if remark and name:
            name = f"{remark}-{name}"
        elif remark:
            name = f"{remark}-Hysteria2-{server}:{port}"
        elif name:
            name = name
        else:
            name = f"Hysteria2-{server}:{port}"
        
        config = {
            'name': name,
            'type': 'hysteria2',
            'server': server,
            'port': port,
            'password': password,
        }
        
        if query_params.get('sni'):
            config['sni'] = query_params['sni'][0]
        
        insecure = query_params.get('insecure', ['0'])[0] == '1' or query_params.get('allowInsecure', ['0'])[0] == '1'
        if insecure:
            config['skip-cert-verify'] = True
        
        if query_params.get('alpn'):
            config['alpn'] = query_params['alpn'][0].split(',')
        
        return clean_config(config)
        
    except Exception as e:
        print(f"  Hysteria2è§£æå¤±è´¥: {e}")
        return None

def parse_ss(url, remark=None):
    """è§£æShadowsocksé“¾æ¥"""
    try:
        url = url[5:]  # ç§»é™¤ ss://
        
        name = ""
        if '#' in url:
            url, fragment = url.split('#', 1)
            name = unquote(fragment)
        
        decoded = safe_decode_base64(url.split('@')[0] if '@' in url else url)
        
        if decoded and ':' in decoded:
            method, password = decoded.split(':', 1)
        else:
            if '@' in url:
                encoded_auth, server_part = url.split('@', 1)
                decoded_auth = safe_decode_base64(encoded_auth)
                if decoded_auth and ':' in decoded_auth:
                    method, password = decoded_auth.split(':', 1)
                else:
                    return None
            else:
                return None
        
        if '@' in url:
            _, server_part = url.split('@', 1)
        else:
            server_part = url
        
        if '?' in server_part:
            server_part, _ = server_part.split('?', 1)
        
        if ':' in server_part:
            server, port = server_part.split(':', 1)
            port = int(port)
        else:
            return None
        
        # æ·»åŠ å¤‡æ³¨å‰ç¼€
        if remark and name:
            name = f"{remark}-{name}"
        elif remark:
            name = f"{remark}-SS-{server}:{port}"
        elif name:
            name = name
        else:
            name = f"SS-{server}:{port}"
        
        config = {
            'name': name,
            'type': 'ss',
            'server': server,
            'port': port,
            'cipher': method,
            'password': password,
            'udp': True
        }
        
        return clean_config(config)
        
    except Exception as e:
        print(f"  SSè§£æå¤±è´¥: {e}")
        return None

def parse_vmess(url, remark=None):
    """è§£æVMessé“¾æ¥"""
    try:
        encoded = url[8:]  # ç§»é™¤ vmess://
        decoded = safe_decode_base64(encoded)
        
        if not decoded:
            return None
        
        vmess_config = json.loads(decoded)
        
        original_name = vmess_config.get('ps', f"VMess-{vmess_config.get('add', 'unknown')}")
        
        # æ·»åŠ å¤‡æ³¨å‰ç¼€
        if remark:
            name = f"{remark}-{original_name}"
        else:
            name = original_name
        
        config = {
            'name': name,
            'type': 'vmess',
            'server': vmess_config.get('add', ''),
            'port': int(vmess_config.get('port', 443)),
            'uuid': vmess_config.get('id', ''),
            'alterId': int(vmess_config.get('aid', 0)),
            'cipher': vmess_config.get('scy', 'auto'),
            'udp': True,
        }
        
        if vmess_config.get('tls') == 'tls':
            config['tls'] = True
            config['skip-cert-verify'] = vmess_config.get('allowInsecure') in [True, 'true', '1']
        
        sni = vmess_config.get('sni') or vmess_config.get('host')
        if sni:
            config['servername'] = sni
        
        network = vmess_config.get('net', 'tcp')
        if network != 'tcp':
            config['network'] = network
            
            if network == 'ws':
                ws_opts = {}
                if vmess_config.get('path'):
                    ws_opts['path'] = vmess_config['path']
                if vmess_config.get('host'):
                    ws_opts['headers'] = {'Host': vmess_config['host']}
                if ws_opts:
                    config['ws-opts'] = ws_opts
        
        return clean_config(config)
        
    except Exception as e:
        print(f"  VMessè§£æå¤±è´¥: {e}")
        return None

def parse_trojan(url, remark=None):
    """è§£æTrojané“¾æ¥"""
    try:
        url = url[9:]  # ç§»é™¤ trojan://
        
        name = ""
        if '#' in url:
            url, fragment = url.split('#', 1)
            name = unquote(fragment)
        
        if '@' in url:
            password_part, server_part = url.split('@', 1)
            password = password_part
        else:
            return None
        
        server = ""
        port = 443
        query_params = {}
        
        if '?' in server_part:
            server_port_part, query_str = server_part.split('?', 1)
            query_params = parse_qs(query_str)
        else:
            server_port_part = server_part
        
        if ':' in server_port_part:
            server, port_str = server_port_part.split(':', 1)
            port = int(port_str)
        else:
            server = server_port_part
        
        # æ·»åŠ å¤‡æ³¨å‰ç¼€
        if remark and name:
            name = f"{remark}-{name}"
        elif remark:
            name = f"{remark}-Trojan-{server}:{port}"
        elif name:
            name = name
        else:
            name = f"Trojan-{server}:{port}"
        
        config = {
            'name': name,
            'type': 'trojan',
            'server': server,
            'port': port,
            'password': password,
            'sni': query_params.get('sni', [''])[0] or server,
            'skip-cert-verify': query_params.get('allowInsecure', ['0'])[0] == '1',
            'udp': True
        }
        
        return clean_config(config)
        
    except Exception as e:
        print(f"  Trojanè§£æå¤±è´¥: {e}")
        return None

def parse_vless(url, remark=None):
    """è§£æVLESSé“¾æ¥"""
    try:
        url = url[8:]  # ç§»é™¤ vless://
        
        name = ""
        if '#' in url:
            url, fragment = url.split('#', 1)
            name = unquote(fragment)
        
        if '@' in url:
            uuid_part, server_part = url.split('@', 1)
            uuid = uuid_part
        else:
            return None
        
        server = ""
        port = 443
        query_params = {}
        
        if '?' in server_part:
            server_port_part, query_str = server_part.split('?', 1)
            query_params = parse_qs(query_str)
        else:
            server_port_part = server_part
        
        if ':' in server_port_part:
            server, port_str = server_port_part.split(':', 1)
            port = int(port_str)
        else:
            server = server_port_part
        
        # æ·»åŠ å¤‡æ³¨å‰ç¼€
        if remark and name:
            name = f"{remark}-{name}"
        elif remark:
            name = f"{remark}-VLESS-{server}:{port}"
        elif name:
            name = name
        else:
            name = f"VLESS-{server}:{port}"
        
        config = {
            'name': name,
            'type': 'vless',
            'server': server,
            'port': port,
            'uuid': uuid,
            'udp': True,
        }
        
        security = query_params.get('security', [''])[0]
        if security in ['tls', 'xtls']:
            config['tls'] = True
            config['skip-cert-verify'] = query_params.get('allowInsecure', ['0'])[0] == '1'
        
        sni = query_params.get('sni', [''])[0] or server
        config['servername'] = sni
        
        return clean_config(config)
        
    except Exception as e:
        print(f"  VLESSè§£æå¤±è´¥: {e}")
        return None

def parse_proxy_url(url, remark=None):
    """è§£æä»£ç†URL"""
    if not url or not isinstance(url, str):
        return None
    
    url = url.strip()
    
    if url.startswith('hysteria2://'):
        return parse_hysteria2(url, remark)
    elif url.startswith('ss://'):
        return parse_ss(url, remark)
    elif url.startswith('vmess://'):
        return parse_vmess(url, remark)
    elif url.startswith('trojan://'):
        return parse_trojan(url, remark)
    elif url.startswith('vless://'):
        return parse_vless(url, remark)
    
    return None

def fetch_subscription(url, timeout=30):
    """è·å–è®¢é˜…å†…å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/plain, */*',
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        content = response.text.strip()
        decoded = safe_decode_base64(content)
        
        if decoded:
            return decoded, True, None
        
        return content, True, None
        
    except requests.exceptions.Timeout:
        return None, False, "è¯·æ±‚è¶…æ—¶"
    except requests.exceptions.ConnectionError:
        return None, False, "è¿æ¥é”™è¯¯"
    except requests.exceptions.HTTPError as e:
        return None, False, f"HTTPé”™è¯¯: {e.response.status_code}"
    except Exception as e:
        return None, False, f"æœªçŸ¥é”™è¯¯: {str(e)}"

def process_subscription_content(content, remark=None):
    """å¤„ç†è®¢é˜…å†…å®¹"""
    if not content:
        return [], 0
    
    # â‘  å…ˆå°è¯•å½“ Clash YAML æ ¼å¼
    try:
        data = yaml.safe_load(content)
        if isinstance(data, dict) and 'proxies' in data:
            proxies = data['proxies']
            node_count = len(proxies)
            
            # æ·»åŠ å¤‡æ³¨
            if remark:
                for p in proxies:
                    p['name'] = f"{remark}-{p.get('name','node')}"
            
            print(f"    æ£€æµ‹åˆ°Clash YAMLæ ¼å¼ï¼Œæ‰¾åˆ° {node_count} ä¸ªèŠ‚ç‚¹")
            # è¿”å›æ‰€æœ‰ä»£ç†èŠ‚ç‚¹å’ŒèŠ‚ç‚¹æ•°é‡
            return proxies, node_count
    except Exception as e:
        print(f"    è§£æ YAML å¤±è´¥ï¼Œå°è¯•URIæ ¼å¼: {e}")
    
    # â‘¡ å¦‚æœä¸æ˜¯ Clash é…ç½®ï¼Œå†å›åˆ°åŸæ¥çš„ URI è§£æé€»è¾‘
    proxies = []
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        
        proxy = parse_proxy_url(line, remark)
        if proxy:
            proxies.append(proxy)
    
    node_count = len(proxies)
    return proxies, node_count

def generate_clash_config_with_groups(all_nodes, proxy_groups, filename, source_content, 
                                     success_count, total_count, failed_urls, remark_stats):
    """ç”Ÿæˆå¸¦åˆ†ç»„åŠŸèƒ½çš„Clashé…ç½® - ç»ˆæç®€åŒ–ç‰ˆ"""
    
    # è·å–å½“å‰æ—¶é—´
    update_time = get_beijing_time()
    
    # ç”Ÿæˆå¤‡æ³¨
    comments = f"""# ========================================
# Clash é…ç½®æ–‡ä»¶ - ç»ˆæç®€åŒ–ç‰ˆ
# ========================================
# 
# æ›´æ–°æ—¶é—´ï¼ˆä¸œå…«åŒºåŒ—äº¬æ—¶é—´ï¼‰: {update_time}
# ä»“åº“åç§°: lzhp529
# è¾“å…¥æºæ–‡ä»¶: {filename}
# è®¢é˜…é“¾æ¥è·å–æƒ…å†µ: {success_count}/{total_count}
# 
# åˆ†ç»„ç»Ÿè®¡:
{remark_stats}
# 
# å¤±è´¥çš„é“¾æ¥:
{failed_urls}
# 
# è¾“å…¥æºæ–‡ä»¶å†…å®¹:
{source_content}
# 
# ========================================
# é…ç½®è¯´æ˜:
# 1. ç»Ÿä¸€ä»£ç†ç«¯å£: 7890 (HTTP/SOCKSæ··åˆ)
# 2. èŠ‚ç‚¹é€‰æ‹©: ä»…åŒ…å«è´Ÿè½½å‡è¡¡ã€è‡ªåŠ¨é€‰æ‹©ã€DIRECT
# 3. è´Ÿè½½å‡è¡¡: é»˜è®¤ç­–ç•¥ï¼Œè‡ªåŠ¨åˆ†é…æµé‡
# 4. è‡ªåŠ¨é€‰æ‹©: é€‰æ‹©æœ€ä½å»¶è¿ŸèŠ‚ç‚¹
# 5. åˆ†ç»„ç­–ç•¥: æŒ‰è®¢é˜…æºåˆ†ç»„ï¼Œæ–¹ä¾¿åˆ‡æ¢
# ========================================
# é…ç½®å¼€å§‹
# ========================================
"""
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•èŠ‚ç‚¹ï¼Œåˆ›å»ºæµ‹è¯•é…ç½®
    if not all_nodes:
        print("  æ²¡æœ‰æœ‰æ•ˆèŠ‚ç‚¹ï¼Œåˆ›å»ºæµ‹è¯•é…ç½®")
        all_nodes = [{
            'name': 'æµ‹è¯•èŠ‚ç‚¹',
            'type': 'ss',
            'server': 'example.com',
            'port': 443,
            'cipher': 'aes-256-gcm',
            'password': 'password',
            'udp': True
        }]
    
    # Clashé…ç½® - ç»ˆæç®€åŒ–ç‰ˆ
    config = {
        'mixed-port': 7890,  # ç»Ÿä¸€ä½¿ç”¨æ··åˆç«¯å£
        'allow-lan': False,
        'mode': 'rule',
        'log-level': 'info',
        'external-controller': '127.0.0.1:9090',
        
        # DNSè®¾ç½®
        'dns': {
            'enable': True,
            'ipv6': False,
            'listen': '127.0.0.1:53',
            'default-nameserver': ['223.5.5.5', '119.29.29.29'],
            'enhanced-mode': 'fake-ip',
            'fake-ip-range': '198.18.0.1/16',
            'nameserver': [
                'https://doh.pub/dns-query',
                'https://dns.alidns.com/dns-query'
            ]
        },
        
        # ä»£ç†èŠ‚ç‚¹
        'proxies': all_nodes[:200],  # æœ€å¤š200ä¸ªèŠ‚ç‚¹
        
        # ç­–ç•¥ç»„ - æåº¦ç®€åŒ–ç‰ˆ
        'proxy-groups': proxy_groups,
        
        # è§„åˆ™ - ç®€åŒ–è·¯ç”±
        'rules': [
            # å›½å†…åŸŸåç›´è¿
            'DOMAIN-SUFFIX,cn,DIRECT',
            'DOMAIN-SUFFIX,baidu.com,DIRECT',
            'DOMAIN-SUFFIX,qq.com,DIRECT',
            'DOMAIN-SUFFIX,taobao.com,DIRECT',
            'DOMAIN-SUFFIX,jd.com,DIRECT',
            'DOMAIN-SUFFIX,weibo.com,DIRECT',
            'DOMAIN-SUFFIX,sina.com,DIRECT',
            'DOMAIN-SUFFIX,163.com,DIRECT',
            'DOMAIN-SUFFIX,alibaba.com,DIRECT',
            'DOMAIN-SUFFIX,alipay.com,DIRECT',
            'DOMAIN-SUFFIX,tencent.com,DIRECT',
            'DOMAIN-SUFFIX,bilibili.com,DIRECT',
            'DOMAIN-SUFFIX,zhihu.com,DIRECT',
            
            # GEOIPä¸­å›½ç›´è¿
            'GEOIP,CN,DIRECT',
            
            # æœ€ç»ˆè§„åˆ™ - ä½¿ç”¨èŠ‚ç‚¹é€‰æ‹©ï¼ˆé»˜è®¤è´Ÿè½½å‡è¡¡ï¼‰
            'MATCH,èŠ‚ç‚¹é€‰æ‹©'
        ]
    }
    
    config = clean_config(config)
    
    # å†™å…¥æ–‡ä»¶
    output_dir = 'è®¢é˜…é“¾æ¥'
    os.makedirs(output_dir, exist_ok=True)
    
    output_path = os.path.join(output_dir, f'{filename}.yaml')
    
    with open(output_path, 'w', encoding='utf-8') as f:
        # å†™å…¥å¤‡æ³¨
        f.write(comments)
        # å†™å…¥é…ç½®
        yaml.dump(config, f, 
                 allow_unicode=True, 
                 default_flow_style=False, 
                 sort_keys=False,
                 width=float("inf"))
    
    print(f"  ç”Ÿæˆé…ç½®æ–‡ä»¶: {output_path}")
    print(f"  åŒ…å« {len(all_nodes[:200])} ä¸ªèŠ‚ç‚¹")
    print(f"  åŒ…å« {len(proxy_groups)} ä¸ªç­–ç•¥ç»„")
    print(f"  ä»£ç†ç«¯å£: 7890 (HTTP/SOCKSæ··åˆ)")
    
    return len(all_nodes[:200])

def build_proxy_groups(all_nodes, remark_nodes_map):
    """æ„å»ºç­–ç•¥ç»„é…ç½® - æåº¦ç®€åŒ–ç‰ˆ"""
    # è·å–æ‰€æœ‰èŠ‚ç‚¹åç§°
    all_node_names = [node.get('name', f'èŠ‚ç‚¹{i+1}') for i, node in enumerate(all_nodes[:200])]
    
    # åŸºç¡€ç­–ç•¥ç»„ - æåº¦ç®€åŒ–ç‰ˆ
    proxy_groups = [
        {
            'name': 'èŠ‚ç‚¹é€‰æ‹©',
            'type': 'select',
            'proxies': ['è´Ÿè½½å‡è¡¡', 'è‡ªåŠ¨é€‰æ‹©', 'DIRECT']  # åªä¿ç•™è¿™3ä¸ªé€‰é¡¹
        },
        {
            'name': 'è´Ÿè½½å‡è¡¡',
            'type': 'load-balance',
            'url': 'http://www.gstatic.com/generate_204',
            'interval': 300,
            'strategy': 'consistent-hashing',
            'proxies': all_node_names
        },
        {
            'name': 'è‡ªåŠ¨é€‰æ‹©',
            'type': 'url-test',
            'url': 'http://www.gstatic.com/generate_204',
            'interval': 300,
            'tolerance': 50,
            'proxies': all_node_names
        }
    ]
    
    # ä¸ºæ¯ä¸ªæœ‰å¤‡æ³¨çš„é“¾æ¥åˆ›å»ºç‹¬ç«‹ç­–ç•¥ç»„
    for remark, nodes in remark_nodes_map.items():
        if remark and nodes:
            node_names = [node.get('name') for node in nodes if node.get('name')]
            if node_names:
                proxy_groups.append({
                    'name': remark,
                    'type': 'url-test',
                    'url': 'http://www.gstatic.com/generate_204',
                    'interval': 300,
                    'tolerance': 50,
                    'proxies': node_names[:50]  # æœ€å¤š50ä¸ªèŠ‚ç‚¹
                })
    
    return proxy_groups

def clear_output_directory():
    """æ¸…ç©ºè¾“å‡ºç›®å½•"""
    output_dir = 'è®¢é˜…é“¾æ¥'
    
    if os.path.exists(output_dir):
        print(f"æ¸…ç©ºè¾“å‡ºç›®å½•: {output_dir}")
        try:
            for filename in os.listdir(output_dir):
                file_path = os.path.join(output_dir, filename)
                try:
                    if os.path.isfile(file_path) or os.path.islink(file_path):
                        os.unlink(file_path)
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                except Exception as e:
                    print(f"åˆ é™¤æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            print("è¾“å‡ºç›®å½•å·²æ¸…ç©º")
        except Exception as e:
            print(f"æ¸…ç©ºç›®å½•å¤±è´¥: {e}")
    else:
        os.makedirs(output_dir, exist_ok=True)
        print("åˆ›å»ºè¾“å‡ºç›®å½•")

def read_source_file_content(filepath, url_results=None):
    """è¯»å–æºæ–‡ä»¶å†…å®¹å¹¶æ·»åŠ #æ³¨é‡Šï¼ŒåŒæ—¶æ·»åŠ èŠ‚ç‚¹æ•°é‡å’Œå¤±è´¥åŸå› ä¿¡æ¯"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        commented_lines = []
        current_remark = None
        url_index = 0
        
        for line in lines:
            line = line.rstrip('\n')
            if line.strip():
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ³¨é‡Šè¡Œ
                if line.startswith('#'):
                    # æå–å¤‡æ³¨
                    remark = extract_remark_from_comment(line)
                    if remark:
                        current_remark = remark
                    commented_lines.append(f"# {line}")
                else:
                    # è¿™æ˜¯URLè¡Œ
                    url_info = None
                    if url_results and url_index < len(url_results):
                        url_info = url_results[url_index]
                        url_index += 1
                    
                    # æ·»åŠ åŸå§‹URL
                    commented_lines.append(f"# {line}")
                    
                    # æ·»åŠ èŠ‚ç‚¹æ•°é‡å’Œå¤±è´¥åŸå› ä¿¡æ¯
                    if url_info:
                        success = url_info.get('success', False)
                        node_count = url_info.get('node_count', 0)
                        error_msg = url_info.get('error_msg', '')
                        
                        if success:
                            if node_count > 0:
                                commented_lines.append(f"# èŠ‚ç‚¹æ•°é‡ï¼š{node_count} ä¸ª")
                            else:
                                commented_lines.append(f"# èŠ‚ç‚¹æ•°é‡ï¼š0 ä¸ª")
                                if error_msg:
                                    commented_lines.append(f"# å¤±è´¥åŸå› ï¼š{error_msg}")
                        else:
                            commented_lines.append(f"# èŠ‚ç‚¹æ•°é‡ï¼šè·å–å¤±è´¥")
                            if error_msg:
                                commented_lines.append(f"# å¤±è´¥åŸå› ï¼š{error_msg}")
                            else:
                                commented_lines.append(f"# å¤±è´¥åŸå› ï¼šæœªçŸ¥é”™è¯¯")
                    else:
                        # å¦‚æœæ²¡æœ‰ç»“æœä¿¡æ¯ï¼Œæ˜¾ç¤ºæœªçŸ¥çŠ¶æ€
                        commented_lines.append(f"# èŠ‚ç‚¹æ•°é‡ï¼šæœªçŸ¥")
                        commented_lines.append(f"# å¤±è´¥åŸå› ï¼šæœªå¤„ç†")
                    
                    # å¦‚æœä¸‹ä¸€ä¸ªè¡Œä¸æ˜¯URLï¼Œé‡ç½®å½“å‰å¤‡æ³¨
                    current_remark = None
            else:
                commented_lines.append("#")
        
        return "\n".join(commented_lines)
        
    except Exception as e:
        print(f"è¯»å–æºæ–‡ä»¶å¤±è´¥: {e}")
        return "# æ— æ³•è¯»å–æºæ–‡ä»¶å†…å®¹"

def generate_remark_stats(remark_stats, failed_urls_list, url_entries):
    """ç”Ÿæˆåˆ†ç»„ç»Ÿè®¡ä¿¡æ¯ï¼Œç»Ÿä¸€æ˜¾ç¤ºæˆåŠŸå’Œå¤±è´¥çš„è®¢é˜…æº"""
    if not remark_stats and not failed_urls_list:
        return "#   æ— åˆ†ç»„ä¿¡æ¯"
    
    stats_lines = ["#   åˆ†ç»„ç»Ÿè®¡:"]
    
    # æŒ‰åŸå§‹é¡ºåºæ˜¾ç¤ºæ‰€æœ‰è®¢é˜…æº
    for entry in url_entries:
        remark = entry['remark']
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
        if remark in remark_stats:
            # æˆåŠŸè®¢é˜…
            count = remark_stats[remark]
            stats_lines.append(f"#   {remark}: {count} ä¸ªèŠ‚ç‚¹")
        else:
            # æŸ¥æ‰¾å¤±è´¥åŸå› 
            error_msg = "æœªçŸ¥é”™è¯¯"
            for failed_info in failed_urls_list:
                if failed_info.get('remark') == remark:
                    error_msg = failed_info.get('error_msg', 'æœªçŸ¥é”™è¯¯')
                    break
            
            # å¤±è´¥è®¢é˜…
            stats_lines.append(f"#   {remark}ï¼šè·å–å¤±è´¥  -  {error_msg}")
    
    return "\n".join(stats_lines)

def generate_failed_urls_comments(failed_urls_list):
    """ç”Ÿæˆå¤±è´¥é“¾æ¥çš„æ³¨é‡Š"""
    if not failed_urls_list:
        return "# æ— å¤±è´¥é“¾æ¥"
    
    failed_comments = []
    for failed_info in failed_urls_list:
        remark = failed_info.get('remark', 'æ— å¤‡æ³¨')
        url = failed_info.get('url', '')
        error_msg = failed_info.get('error_msg', 'æœªçŸ¥é”™è¯¯')
        
        # æ¯ä¸ªå¤±è´¥é“¾æ¥å•ç‹¬æ˜¾ç¤ºï¼ŒåŒ…å«å¤‡æ³¨åç§°å’Œå¤±è´¥åŸå› 
        failed_comments.append(f"# ã€{remark}ã€‘")
        failed_comments.append(f"# {url}")
        failed_comments.append(f"# å¤±è´¥åŸå› ï¼š{error_msg}")
        failed_comments.append(f"#")
    
    return "\n".join(failed_comments)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("è‡ªåŠ¨è®¢é˜…ç”Ÿæˆå™¨ - ç»ˆæç®€åŒ–ç‰ˆ")
    print("ä»“åº“: lzhp529")
    print("=" * 70)
    print(f"å¼€å§‹æ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰: {get_beijing_time()}")
    
    # æ¸…ç©ºè¾“å‡ºç›®å½•
    clear_output_directory()
    
    input_dir = 'è¾“å…¥æº'
    os.makedirs(input_dir, exist_ok=True)
    
    # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
    txt_files = [f for f in os.listdir(input_dir) if f.endswith('.txt')]
    
    if not txt_files:
        print(f"\næ²¡æœ‰æ‰¾åˆ°è¾“å…¥æ–‡ä»¶ï¼Œè¯·åœ¨ '{input_dir}' ä¸­åˆ›å»º.txtæ–‡ä»¶")
        print("åˆ›å»ºç¤ºä¾‹æ–‡ä»¶...")
        example_content = """# ç»´äº‘äº‘ 2å¹´åˆ°æœŸ
https://vyy.cqsvhb.cn/s/c59454c04c7395f58b5d8165a598ad64

# æœºåœºA é«˜é€Ÿç¨³å®š
https://example.com/subscribe1.txt

# å…è´¹èŠ‚ç‚¹
https://example.com/free.txt
"""
        with open(os.path.join(input_dir, 'example.txt'), 'w', encoding='utf-8') as f:
            f.write(example_content)
        print(f"å·²åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {input_dir}/example.txt")
        txt_files = ['example.txt']
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for filename in txt_files:
        print(f"\n" + "=" * 50)
        print(f"å¤„ç†æ–‡ä»¶: {filename}")
        print("=" * 50)
        
        filepath = os.path.join(input_dir, filename)
        
        # è§£ææºæ–‡ä»¶ï¼Œæå–å¸¦å¤‡æ³¨çš„é“¾æ¥
        url_entries = parse_source_file(filepath)
        
        if not url_entries:
            print("  æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé“¾æ¥")
            continue
        
        total_count = len(url_entries)
        print(f"  æ‰¾åˆ° {total_count} ä¸ªå¸¦å¤‡æ³¨çš„é“¾æ¥")
        
        # ç»Ÿè®¡ä¿¡æ¯
        all_proxies = []
        failed_urls_list = []  # å­˜å‚¨å¤±è´¥é“¾æ¥çš„è¯¦ç»†ä¿¡æ¯
        success_count = 0
        remark_nodes_map = {}  # æŒ‰å¤‡æ³¨åˆ†ç»„çš„èŠ‚ç‚¹
        remark_stats = {}      # åˆ†ç»„ç»Ÿè®¡
        url_results = []       # å­˜å‚¨æ¯ä¸ªURLçš„å¤„ç†ç»“æœ
        
        # å¤„ç†æ¯ä¸ªé“¾æ¥
        for i, entry in enumerate(url_entries):
            url = entry['url']
            remark = entry['remark']
            
            print(f"\n  [{i+1}/{total_count}] å¤„ç†é“¾æ¥")
            print(f"    é“¾æ¥: {url[:80]}...")
            if remark:
                print(f"    å¤‡æ³¨: {remark}")
            
            result = fetch_subscription(url, timeout=15)
            content, success, error_msg = result
            
            # å­˜å‚¨å¤„ç†ç»“æœ
            url_result = {
                'url': url,
                'remark': remark,
                'success': success,
                'error_msg': error_msg if not success else '',
                'node_count': 0
            }
            
            if success and content:
                proxies, node_count = process_subscription_content(content, remark)
                url_result['node_count'] = node_count
                
                if proxies:
                    all_proxies.extend(proxies)
                    success_count += 1
                    
                    # æŒ‰å¤‡æ³¨åˆ†ç»„
                    if remark:
                        if remark not in remark_nodes_map:
                            remark_nodes_map[remark] = []
                        remark_nodes_map[remark].extend(proxies)
                        
                        # æ›´æ–°ç»Ÿè®¡
                        remark_stats[remark] = remark_stats.get(remark, 0) + len(proxies)
                    
                    print(f"    âœ… æˆåŠŸè·å–ï¼Œæ‰¾åˆ° {len(proxies)} ä¸ªèŠ‚ç‚¹")
                else:
                    print(f"    âš ï¸ è·å–æˆåŠŸä½†æœªæ‰¾åˆ°æœ‰æ•ˆèŠ‚ç‚¹")
                    url_result['error_msg'] = 'æ— æœ‰æ•ˆèŠ‚ç‚¹'
                    failed_urls_list.append(url_result)
            else:
                error_info = error_msg if error_msg else "æœªçŸ¥é”™è¯¯"
                print(f"    âŒ å¤±è´¥: {error_info}")
                url_result['error_msg'] = error_info
                failed_urls_list.append(url_result)
            
            url_results.append(url_result)
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if i < total_count - 1:
                time.sleep(1)
        
        # è¯»å–æºæ–‡ä»¶å†…å®¹ï¼ŒåŒ…å«èŠ‚ç‚¹æ•°é‡å’Œå¤±è´¥åŸå› ä¿¡æ¯
        source_content = read_source_file_content(filepath, url_results)
        
        # ç”Ÿæˆåˆ†ç»„ç»Ÿè®¡å’Œå¤±è´¥é“¾æ¥ä¿¡æ¯
        remark_stats_comments = generate_remark_stats(remark_stats, failed_urls_list, url_entries)
        
        # ç”Ÿæˆå¤±è´¥é“¾æ¥æ³¨é‡Š
        failed_comments = generate_failed_urls_comments(failed_urls_list)
        
        # å»é‡
        unique_proxies = []
        seen = set()
        
        for proxy in all_proxies:
            if not proxy:
                continue
            
            key = f"{proxy.get('server', '')}:{proxy.get('port', '')}:{proxy.get('type', '')}:{proxy.get('name', '')}"
            if key not in seen:
                seen.add(key)
                unique_proxies.append(proxy)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\n  {'='*30}")
        print(f"  å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"    æ€»é“¾æ¥æ•°: {total_count}")
        print(f"    æˆåŠŸè·å–: {success_count}")
        print(f"    å¤±è´¥é“¾æ¥: {total_count - success_count}")
        print(f"    åŸå§‹èŠ‚ç‚¹: {len(all_proxies)} ä¸ª")
        print(f"    å»é‡èŠ‚ç‚¹: {len(unique_proxies)} ä¸ª")
        
        # åˆ†ç»„ç»Ÿè®¡
        if remark_stats:
            print(f"    åˆ†ç»„èŠ‚ç‚¹åˆ†å¸ƒ:")
            for remark, count in sorted(remark_stats.items()):
                print(f"      {remark}: {count} ä¸ª")
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for proxy in unique_proxies:
            proxy_type = proxy.get('type', 'unknown')
            type_stats[proxy_type] = type_stats.get(proxy_type, 0) + 1
        
        if type_stats:
            print(f"    èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
            for proxy_type, count in sorted(type_stats.items()):
                print(f"      {proxy_type}: {count} ä¸ª")
        
        # æ„å»ºç­–ç•¥ç»„
        proxy_groups = build_proxy_groups(unique_proxies, remark_nodes_map)
        
        # ç”Ÿæˆé…ç½®
        if unique_proxies:
            base_name = os.path.splitext(filename)[0]
            node_count = generate_clash_config_with_groups(
                unique_proxies, 
                proxy_groups,
                base_name, 
                source_content,
                success_count,
                total_count,
                failed_comments,
                remark_stats_comments
            )
            print(f"\n    âœ… é…ç½®æ–‡ä»¶ç”ŸæˆæˆåŠŸ")
            print(f"    ğŸ“Š ä»£ç†èŠ‚ç‚¹: {node_count} ä¸ª")
            print(f"    ğŸ·ï¸  åˆ†ç»„ç­–ç•¥ç»„: {len(remark_nodes_map)} ä¸ª")
            print(f"    âš–ï¸  é»˜è®¤ç­–ç•¥: è´Ÿè½½å‡è¡¡")
            print(f"    ğŸ”Œ ä»£ç†ç«¯å£: 7890")
        else:
            print("\n    âš ï¸ æ²¡æœ‰æœ‰æ•ˆèŠ‚ç‚¹ï¼Œç”Ÿæˆç©ºé…ç½®")
            # ç”Ÿæˆä¸€ä¸ªç©ºé…ç½®ï¼Œä½†ä»ç„¶åŒ…å«å¤‡æ³¨
            empty_proxies = []
            empty_groups = build_proxy_groups([], {})
            base_name = os.path.splitext(filename)[0]
            generate_clash_config_with_groups(
                empty_proxies,
                empty_groups,
                base_name,
                source_content,
                success_count,
                total_count,
                failed_comments,
                remark_stats_comments
            )
    
    print(f"\n" + "=" * 70)
    print(f"ç”Ÿæˆå®Œæˆï¼")
    print(f"å®Œæˆæ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰: {get_beijing_time()}")
    print("=" * 70)

if __name__ == '__main__':
    main()
